### Шанс вопроса: 14%

Линейные модели обычно обучают методом наименьших квадратов (OLS), который минимизирует сумму квадратов ошибок между предсказанными и фактическими значениями. В общем виде для линейной регрессии с n признаками это можно представить как:

\[ \hat{y} = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \ldots + \beta_n x_n \]

Где \( \hat{y} \) — предсказанное значение, \( \beta_0 \) — смещение (intercept), а \( \beta_1, \beta_2, \ldots, \beta_n \) — коэффициенты при признаках.

Алгоритм обучения включает следующие шаги:

1. **Инициализация параметров**: Начальные значения для \( \beta_0, \beta_1, \ldots, \beta_n \) выбираются случайным образом или устанавливаются в 0.
2. **Вычисление предсказаний**: Для заданных параметров и данных вычисляются предсказанные значения \( \hat{y} \).
3. **Оценка ошибки**: Рассчитывается сумма квадратов ошибок (SSE) между фактическими и предсказанными значениями.
4. **Градиентный спуск**: Для минимизации SSE выполняются обновления параметров \( \beta \) в направлении антиградиента функции ошибки. Это делается по формуле:
   \[ \beta_i = \beta_i - \eta \frac{\partial}{\partial \beta_i} (SSE) \]
   Где \( \eta \) — скорость обучения (learning rate).
5. **Повторение**: Шаги 2-4 повторяются до тех пор, пока не будет достигнута сходимость или выполнено заданное количество итераций.

Пример кода на Python для линейной регрессии с использованием библиотеки `scikit-learn`:

```python
import numpy as np
from sklearn.linear_model import LinearRegression

# Пример данных
X = np.array([[1], [2], [3], [4]])  # Матрица признаков
y = np.array([2, 3, 4, 5])          # Вектор целевых значений

# Создание и обучение модели
model = LinearRegression()
model.fit(X, y)

# Предсказание
predictions = model.predict(X)

print("Коэффициенты:", model.coef_)
print("Смещение (intercept):", model.intercept_)
```

Этот код создает линейную регрессионную модель, обучает её на заданных данных и выводит найденные коэффициенты модели.