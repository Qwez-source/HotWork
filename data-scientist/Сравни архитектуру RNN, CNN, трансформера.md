### Шанс вопроса: 14%

В области обработки естественного языка (NLP) три архитектуры — RNN (рекуррентные нейронные сети), CNN (конволюционные нейронные сети) и трансформеры — часто используются для различных задач. Каждая из этих архитектур имеет свои преимущества и недостатки, и выбор между ними зависит от конкретной задачи и доступных ресурсов.

1. **RNN (рекуррентные нейронные сети)**:
   - **Архитектура**: RNN обрабатывает последовательность данных поэтапно, сохраняя скрытое состояние на каждом шаге, которое учитывает предыдущие входные данные.
   - **Пример**: В задаче машинного перевода RNN может быть использован для последовательной обработки слов из одного языка и преобразования их в последовательность слов на другом языке.
   - **Преимущества**: Хорошо подходит для работы с неструктурированными данными, такими как текст или речь, благодаря своей способности сохранять контекст.
   - **Недостатки**: Медленная обучаемость из-за последовательной обработки и трудность в параллелизме, что ограничивает его использование для длинных последовательностей.

2. **CNN (конволюционные нейронные сети)**:
   - **Архитектура**: CNN обрабатывает данные с использованием фильтров, которые сканируют входные данные на разных масштабах и выделяют локальные признаки.
   - **Пример**: В NLP CNN может быть использован для извлечения признаков из фрагментов текста, таких как n-граммы, что помогает в классификации или токенизации текста.
   - **Преимущества**: Хорошо подходит для обработки структурированных данных и имеет хорошую производительность благодаря параллельным вычислениям в фильтрах.
   - **Недостатки**: Менее эффективно при работе с неструктурированными данными, такими как текст, из-за ограниченного контекста, который они могут учитывать.

3. **Трансформеры (Transformer)**:
   - **Архитектура**: Трансформер использует механизм самовнимания (self-attention), позволяющий обрабатывать всю последовательность данных за один проход, что делает его эффективным для обработки длинных последовательностей.
   - **Пример**: В задаче машинного перевода трансформер может быть использован для более эффективной обработки всего текста сразу, учитывая взаимосвязи между словами в предложении.
   - **Преимущества**: Высокая производительность и масштабируемость благодаря параллельной обработке всех слов в последовательности.
   - **Недостатки**: Требует больших объёмов данных для обучения из-за сложности модели и потенциально высокие требования к ресурсам.

Выбор между этими архитектурами зависит от конкретной задачи и доступных данных:
- **Для коротких последовательностей** или **когда важен контекст**: RNN или его современные версии, такие как LSTM или GRU.
- **Для более длинных последовательностей** и **больших объёмов данных**: трансформеры.
- **Для обработки структурированных данных** с высокой степенью параллелизма: CNN.

Таким образом, каждая архитектура имеет свою нишу и может быть эффективно использована в зависимости от особенностей задачи и доступных ресурсов.