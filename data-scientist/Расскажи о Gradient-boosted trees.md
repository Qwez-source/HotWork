### Шанс вопроса: 14%

Gradient-Boosted Trees, также известные как градиентный бустинг деревьев решений, являются мощной моделью машинного обучения, которая использует ансамбль слабосвязанных моделей для улучшения предсказательной способности. Они относятся к семейству методов машинного обучения, основанных на градиентном бустинге.

### Основные компоненты Gradient-Boosted Trees:
1. **Базовые Модели**: Обычно это слабые модели, такие как деревья решений с ограниченной глубиной (например, деревья глубиной 2 или 3 узла).
2. **Ошибка Предсказания**: На каждой итерации модели вычисляется ошибка предсказания между фактическим значением и предсказанным значением на предыдущем шаге.
3. **Регрессор Ошибок**: Добавляется новый регрессор, который пытается уменьшить оставшуюся ошибку. Этот процесс повторяется многократно, пока не будет достигнуто приемлемое качество предсказаний или не будет достигнуто максимальное количество итераций.
4. **Веса Моделей**: Каждая модель в ансамбле получает вес, который определяет её влияние на окончательное предсказание. Веса обычно вычисляются на основе уменьшения ошибки.

### Пример кода на Python с использованием библиотеки `XGBoost` (eXtreme Gradient Boosting):
```python
import xgboost as xgb
from sklearn.datasets import make_classification
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score

# Создание синтетического датасета
X, y = make_classification(n_samples=1000, n_features=20, random_state=42)

# Разделение данных на обучающую и тестовую выборки
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# Создание DMatrix для оптимизации работы с данными в библиотеке xgboost
dtrain = xgb.DMatrix(X_train, label=y_train)
dtest = xgb.DMatrix(X_test, label=y_test)

# Параметры модели
params = {
    'objective': 'binary:logistic',  # Для бинарной классификации
    'max_depth': 4,                   # Максимальная глубина дерева
    'eta': 0.1,                       # Скорость обучения (learning rate)
    'eval_metric': 'logloss'          # Метрика оценки качества модели
}

# Обучение модели
num_rounds = 100
bst = xgb.train(params, dtrain, num_rounds)

# Предсказание на тестовой выборке
preds = bst.predict(dtest)
pred_labels = (preds > 0.5).astype(int)  # Преобразование вероятностей в бинарные метки

# Оценка точности модели
accuracy = accuracy_score(y_test, pred_labels)
print(f"Accuracy: {accuracy}")
```

### Преимущества Gradient-Boosted Trees:
1. **Мощная предсказательная способность**: Может достигать высокой точности предсказаний даже в сложных задачах.
2. **Выбор базовых моделей**: Легко интегрировать с различными слабыми моделями, такими как деревья решений или линейные модели.
3. **Робастность к переобучению**: Благодаря возможности регулировки количества итераций и параметров моделей, легко избежать переобучения.
4. **Вычислительная эффективность**: Эффективно работают на больших объемах данных благодаря параллельным вычислениям и оптимизации.

Gradient-Boosted Trees являются ключевой технологией в области машинного обучения, широко используемой для решения задач классификации, регрессии и многоклассовой классификации.