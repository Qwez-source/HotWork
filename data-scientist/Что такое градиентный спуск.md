### Шанс вопроса: 14%

Градиентный спуск — это алгоритм оптимизации, который применяется для минимизации функций. Он является ключевым инструментом в машинном обучении и глубоком обучении для настройки параметров моделей. Градиентный спуск работает, повторяя следующие шаги:

1. Вычисляет производную (градиент) функции потерь относительно каждого параметра модели.
2. Обновляет параметры в направлении, противоположном градиенту, с некоторым шагом обучения.

Этот процесс продолжается до тех пор, пока функция потерь не достигнет минимума или пока не будет выполнено заданное количество итераций.

### Пример кода на Python для градиентного спуска:

```python
import numpy as np

# Определение функции потерь (например, MSE - Mean Squared Error)
def loss_function(X, y, theta):
    m = len(y)
    predictions = X.dot(theta)
    return (1/(2*m)) * np.sum((predictions - y)**2)

# Градиентный спуск
def gradient_descent(X, y, theta, learning_rate, num_iterations):
    m = len(y)
    for _ in range(num_iterations):
        predictions = X.dot(theta)
        error = predictions - y
        gradient = (1/m) * X.T.dot(error)
        theta -= learning_rate * gradient
    return theta

# Пример данных
X = np.array([[1, 2], [1, 3], [1, 4]])
y = np.array([1, 2, 3])
theta = np.zeros(X.shape[1])
learning_rate = 0.01
num_iterations = 1000

# Запуск градиентного спуска
optimal_theta = gradient_descent(X, y, theta, learning_rate, num_iterations)
print("Оптимальные параметры: ", optimal_theta)
```

В этом примере мы определяем функцию потерь (MSE), а затем реализуем градиентный спуск для нахождения оптимальных параметров модели.