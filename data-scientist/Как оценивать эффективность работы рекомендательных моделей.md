### Шанс вопроса: 14%

Оценка эффективности работы рекомендательных моделей может быть проведена с использованием нескольких ключевых метрик, в зависимости от типа и цели модели. Вот основные подходы:

1. **Точность (Accuracy)**: Эта метрика измеряет долю правильно предсказанных результатов. Однако она может быть неэффективной для данных с дисбалансом классов, так как модель может просто предсказывать наиболее часто встречающийся класс.
   
2. **Точность и Полнота (Precision and Recall)**: Precision измеряет долю истинно-положительных результатов среди всех положительных предсказаний, а Recall — это доля истинно-положительных результатов среди всех фактических положительных случаев. Эти метрики полезны для задач с несбалансированной классификацией.
   ```python
   from sklearn.metrics import precision_score, recall_score

   y_true = [0, 1, 0, 1, 1]
   y_pred = [0, 1, 0, 0, 1]

   precision = precision_score(y_true, y_pred)
   recall = recall_score(y_true, y_pred)
   ```

3. **F1-Score**: Это гармоническое среднее между Precision и Recall, которое учитывает баланс между ними. Он полезен для задач, где необходимо достичь компромисса между этими двумя показателями.
   ```python
   from sklearn.metrics import f1_score

   f1 = f1_score(y_true, y_pred)
   ```

4. **Матрица ошибок (Confusion Matrix)**: Позволяет визуализировать количество правильных и неправильных предсказаний для каждого класса. Это базовый инструмент для понимания работы модели.
   ```python
   from sklearn.metrics import confusion_matrix
   import matplotlib.pyplot as plt

   cm = confusion_matrix(y_true, y_pred)
   plt.imshow(cm, cmap='Blues')
   plt.colorbar()
   plt.xlabel('Предсказано')
   plt.ylabel('Фактически')
   ```

5. **ROC-AUC (Receiver Operating Characteristic - Area Under Curve)**: Эта метрика показывает, насколько хорошо модель различает между классами. AUC равный 0.5 означает случайного угадывания, а значение выше 0.7 обычно считается приемлемым для рекомендательных систем.
   ```python
   from sklearn.metrics import roc_auc_score

   y_true = [0, 1, 0, 1, 1]
   y_scores = [0.2, 0.8, 0.3, 0.9, 0.6]

   auc_roc = roc_auc_score(y_true, y_scores)
   ```

6. **Средняя точность (Mean Average Precision - MAP)**: Эта метрика полезна для ранжирования задач и учитывает позицию элементов в списке предсказаний.
   ```python
   from sklearn.metrics import average_precision_score

   y_true = [0, 1, 0, 1, 1]
   y_scores = [0.2, 0.8, 0.3, 0.9, 0.6]

   map_score = average_precision_score(y_true, y_scores)
   ```

7. **Нормализованная дисконтированная прибыль (Normalized Discounted Cumulative Gain - NDCG)**: Применяется для ранжирования результатов, где более высокий ранг означает большую ценность.
   ```python
   import numpy as np

   def ndcg(y_true, y_pred):
       idcg = np.sum((2**np.array(y_true) - 1) / np.log2(np.arange(2, len(y_true) + 2)))
       dcg = np.sum((2**np.array(y_pred) - 1) / np.log2(np.arange(2, len(y_pred) + 2)))
       return dcg / idcg

   y_true = [3, 2, 3, 0]
   y_pred = [2, 3, 1, 0]

   ndcg_score = ndcg(y_true, y_pred)
   ```

Эти метрики помогают оценить как общую точность модели, так и её способность различать между элементами с разным уровнем релевантности. Выбор конкретной метрики зависит от особенностей задачи и целей проекта.