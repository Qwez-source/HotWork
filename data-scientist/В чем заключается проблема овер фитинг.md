### Шанс вопроса: 14%

Оверфиттинг (overfitting) — это явление, когда модель обучена слишком хорошо на обучающем наборе данных, что приводит к низкой производительности на новых, невидимых данных. Это происходит, когда модель запоминает, а не обобщает закономерности в данных. Заключается проблема в том, что модель становится слишком сложной для обучающего набора и начинает переобучаться.

Пример оверфиттинга можно увидеть на графике, где модель показывает отличные результаты на обучающем наборе (как правило, высокое значение метрики, такой как точность), но плохие результаты на тестовом наборе.

Для борьбы с оверфиттингом можно использовать следующие методы:
1. **Увеличение размера обучающего набора**: Чем больше данных, тем сложнее переобучить модель.
2. **Регуляризация**: Это метод ограничения сложности модели путем добавления штрафных членов в функцию потерь. Примеры регуляризации включают L1 (lasso), L2 (ridge) и dropout в нейронных сетях.
3. **Уменьшение числа признаков**: Удаление некоторых признаков, которые могут быть шумовыми или избыточными.
4. **Early Stopping**: Прерывание обучения модели до того, как она достигнет нулевой потери на обучении, что может указывать на переобучение.
5. **Использование кросс-валидации**: Чтобы оценить модель на более стабильном значении метрики, не зависящем от случайного разбиения данных.

Пример кода для регуляризации в нейронных сетях с использованием dropout:
```python
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout

model = Sequential()
model.add(Dense(64, input_dim=10, activation='relu'))
model.add(Dropout(0.5))  # Добавляем dropout после полносвязного слоя
model.add(Dense(1, activation='sigmoid'))

model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])
```

Этот пример демонстрирует добавление dropout слоя между двумя полносвязными слоями в нейронной сети, что помогает предотвратить переобучение.