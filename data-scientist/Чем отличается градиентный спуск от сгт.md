### Шанс вопроса: 14%

Градиентный спуск (Gradient Descent, GD) и стохастический градиентный спуск (Stochastic Gradient Descent, SGD) — это два распространенных алгоритма оптимизации для обучения моделей машинного обучения.

**Градиентный спуск:**
- **Описание**: Градиентный спуск является общим методом оптимизации, который использует градиент (производную) функции для нахождения локального минимума. Он применяется к каждому параметру модели отдельно.
- **Процесс**: На каждом шаге алгоритм обновляет параметры в направлении наискорейшего спуска, умножая градиент функции потерь на маленький коэффициент обучения (learning rate).
- **Пример кода**:
  ```python
  def gradient_descent(grad_func, params, learning_rate=0.1):
      for _ in range(num_iterations):
          grad = grad_func(params)
          params -= learning_rate * grad
  ```

**Стохастический градиентный спуск:**
- **Описание**: Стохастический градиентный спуск (SGD) представляет собой вариант градиентного спуска, где вместо использования всего набора данных для вычисления градиента, случайным образом выбираются небольшие подмножества данных.
- **Процесс**: На каждом шаге алгоритм обновляет параметры только на основе одного или нескольких случайно выбранных примеров из обучающей выборки. Это делает метод более эффективным для больших данных и позволяет быстрее достичь локального минимума.
- **Пример кода**:
  ```python
  def sgd(grad_func, params, learning_rate=0.1):
      for _ in range(num_iterations):
          sample = get_random_sample()
          grad = grad_func(params, sample)
          params -= learning_rate * grad
  ```

**Основное отличие**: Градиентный спуск обновляет параметры на основе всего набора данных на каждом шаге, тогда как SGD использует случайно выбранные подмножества данных для ускорения процесса обучения и более эффективного поиска минимума.