### Шанс вопроса: 14%

Batch normalization (Batch Norm или BN) — это техника, которая обычно применяется после полносвязных слоев или сверточных слоев нейронной сети и перед激活函数. Она нормализует входные данные для каждого mini-batch (небольшой партии данных), что делает процесс обучения более стабильным и быстрым.

Основная цель Batch Norm — это ускорение обучения глубоких нейронных сетей за счет:
1. **Стабилизации скорости обучения**: Нормализация входов каждого слоя делает градиенты более стабильными, что позволяет использовать более крупные шаги обучения (learning rate), не боясь расходиться.
2. **Уменьшение зависимости от инициализации весов**: Batch Norm помогает избежать проблемы исчезающих и взрывающихся градиентов, делая веса более "бесшовными".
3. **Сокращение времени обучения**: За счет стабилизации скорости обучения и устранения необходимости медленного подбора гиперпараметров, Batch Norm может сократить время обучения в несколько раз.

**Как работает Batch Norm**:
- На каждом шаге обучения для текущего mini-batch вычисляются среднее значение и дисперсия (μ, σ).
- Затем значения нормализуются: \( x' = \frac{x - \mu}{\sqrt{\sigma^2 + \epsilon}} \)
- После этого применяется масштаб (γ) и сдвиг (β): \( y = γx' + β \)
  
Где:
- \( x \) — входные данные.
- \( μ \) — среднее значение по mini-batch.
- \( σ^2 \) — дисперсия по mini-batch.
- \( \epsilon \) — маленькое число для стабилизации (обычно близко к нулю).
- \( γ \) и \( β \) — обучаемые параметры, которые позволяют сети адаптировать нормализованные значения.

**Пример в PyTorch**:
```python
import torch
import torch.nn as nn

# Создание слоя Batch Norm
batch_norm = nn.BatchNorm1d(num_features=64)  # num_features - размерность входных данных

# Предположим, у нас есть данные data с формой (batch_size, 64)
data = torch.randn(32, 64)  # Случайные данные для примера

# Нормализация данных
normalized_data = batch_norm(data)
print(normalized_data)
```

Этот код создает слой Batch Norm с 64 признаками (например, после полносвязного слоя) и применяет его к входным данным. Нормализованные данные затем можно использовать для обучения нейронной сети.

Таким образом, Batch Norm играет важную роль в ускорении и стабилизации процесса обучения глубоких нейронных сетей.